// Code generated by command: go run gen.go -avx. DO NOT EDIT.

#include "textflag.h"

DATA prime_avx<>+0(SB)/4, $0x9e3779b1
GLOBL prime_avx<>(SB), RODATA|NOPTR, $4

// func accumAVX2(acc *[8]uint64, data *byte, key *byte, len uint64)
// Requires: AVX, AVX2
TEXT Â·accumAVX2(SB), NOSPLIT, $0-32
	MOVQ         acc+0(FP), AX
	MOVQ         data+8(FP), CX
	MOVQ         key+16(FP), DX
	MOVQ         key+16(FP), BX
	MOVQ         len+24(FP), BP
	VMOVDQU      (AX), Y1
	VMOVDQU      32(AX), Y2
	VPBROADCASTQ prime_avx<>+0(SB), Y0

accum_large:
	CMPQ     BP, $0x00000400
	JLE      accum
	VMOVDQU  (CX), Y3
	VMOVDQU  (DX), Y4
	VPXOR    Y3, Y4, Y4
	VPSHUFD  $0x31, Y4, Y5
	VPMULUDQ Y4, Y5, Y4
	VPSHUFD  $0x4e, Y3, Y3
	VPADDQ   Y1, Y3, Y1
	VPADDQ   Y1, Y4, Y1
	VMOVDQU  32(CX), Y3
	VMOVDQU  32(DX), Y4
	VPXOR    Y3, Y4, Y4
	VPSHUFD  $0x31, Y4, Y5
	VPMULUDQ Y4, Y5, Y4
	VPSHUFD  $0x4e, Y3, Y3
	VPADDQ   Y2, Y3, Y2
	VPADDQ   Y2, Y4, Y2
	VMOVDQU  64(CX), Y3
	VMOVDQU  8(DX), Y4
	VPXOR    Y3, Y4, Y4
	VPSHUFD  $0x31, Y4, Y5
	VPMULUDQ Y4, Y5, Y4
	VPSHUFD  $0x4e, Y3, Y3
	VPADDQ   Y1, Y3, Y1
	VPADDQ   Y1, Y4, Y1
	VMOVDQU  96(CX), Y3
	VMOVDQU  40(DX), Y4
	VPXOR    Y3, Y4, Y4
	VPSHUFD  $0x31, Y4, Y5
	VPMULUDQ Y4, Y5, Y4
	VPSHUFD  $0x4e, Y3, Y3
	VPADDQ   Y2, Y3, Y2
	VPADDQ   Y2, Y4, Y2
	VMOVDQU  128(CX), Y3
	VMOVDQU  16(DX), Y4
	VPXOR    Y3, Y4, Y4
	VPSHUFD  $0x31, Y4, Y5
	VPMULUDQ Y4, Y5, Y4
	VPSHUFD  $0x4e, Y3, Y3
	VPADDQ   Y1, Y3, Y1
	VPADDQ   Y1, Y4, Y1
	VMOVDQU  160(CX), Y3
	VMOVDQU  48(DX), Y4
	VPXOR    Y3, Y4, Y4
	VPSHUFD  $0x31, Y4, Y5
	VPMULUDQ Y4, Y5, Y4
	VPSHUFD  $0x4e, Y3, Y3
	VPADDQ   Y2, Y3, Y2
	VPADDQ   Y2, Y4, Y2
	VMOVDQU  192(CX), Y3
	VMOVDQU  24(DX), Y4
	VPXOR    Y3, Y4, Y4
	VPSHUFD  $0x31, Y4, Y5
	VPMULUDQ Y4, Y5, Y4
	VPSHUFD  $0x4e, Y3, Y3
	VPADDQ   Y1, Y3, Y1
	VPADDQ   Y1, Y4, Y1
	VMOVDQU  224(CX), Y3
	VMOVDQU  56(DX), Y4
	VPXOR    Y3, Y4, Y4
	VPSHUFD  $0x31, Y4, Y5
	VPMULUDQ Y4, Y5, Y4
	VPSHUFD  $0x4e, Y3, Y3
	VPADDQ   Y2, Y3, Y2
	VPADDQ   Y2, Y4, Y2
	VMOVDQU  256(CX), Y3
	VMOVDQU  32(DX), Y4
	VPXOR    Y3, Y4, Y4
	VPSHUFD  $0x31, Y4, Y5
	VPMULUDQ Y4, Y5, Y4
	VPSHUFD  $0x4e, Y3, Y3
	VPADDQ   Y1, Y3, Y1
	VPADDQ   Y1, Y4, Y1
	VMOVDQU  288(CX), Y3
	VMOVDQU  64(DX), Y4
	VPXOR    Y3, Y4, Y4
	VPSHUFD  $0x31, Y4, Y5
	VPMULUDQ Y4, Y5, Y4
	VPSHUFD  $0x4e, Y3, Y3
	VPADDQ   Y2, Y3, Y2
	VPADDQ   Y2, Y4, Y2
	VMOVDQU  320(CX), Y3
	VMOVDQU  40(DX), Y4
	VPXOR    Y3, Y4, Y4
	VPSHUFD  $0x31, Y4, Y5
	VPMULUDQ Y4, Y5, Y4
	VPSHUFD  $0x4e, Y3, Y3
	VPADDQ   Y1, Y3, Y1
	VPADDQ   Y1, Y4, Y1
	VMOVDQU  352(CX), Y3
	VMOVDQU  72(DX), Y4
	VPXOR    Y3, Y4, Y4
	VPSHUFD  $0x31, Y4, Y5
	VPMULUDQ Y4, Y5, Y4
	VPSHUFD  $0x4e, Y3, Y3
	VPADDQ   Y2, Y3, Y2
	VPADDQ   Y2, Y4, Y2
	VMOVDQU  384(CX), Y3
	VMOVDQU  48(DX), Y4
	VPXOR    Y3, Y4, Y4
	VPSHUFD  $0x31, Y4, Y5
	VPMULUDQ Y4, Y5, Y4
	VPSHUFD  $0x4e, Y3, Y3
	VPADDQ   Y1, Y3, Y1
	VPADDQ   Y1, Y4, Y1
	VMOVDQU  416(CX), Y3
	VMOVDQU  80(DX), Y4
	VPXOR    Y3, Y4, Y4
	VPSHUFD  $0x31, Y4, Y5
	VPMULUDQ Y4, Y5, Y4
	VPSHUFD  $0x4e, Y3, Y3
	VPADDQ   Y2, Y3, Y2
	VPADDQ   Y2, Y4, Y2
	VMOVDQU  448(CX), Y3
	VMOVDQU  56(DX), Y4
	VPXOR    Y3, Y4, Y4
	VPSHUFD  $0x31, Y4, Y5
	VPMULUDQ Y4, Y5, Y4
	VPSHUFD  $0x4e, Y3, Y3
	VPADDQ   Y1, Y3, Y1
	VPADDQ   Y1, Y4, Y1
	VMOVDQU  480(CX), Y3
	VMOVDQU  88(DX), Y4
	VPXOR    Y3, Y4, Y4
	VPSHUFD  $0x31, Y4, Y5
	VPMULUDQ Y4, Y5, Y4
	VPSHUFD  $0x4e, Y3, Y3
	VPADDQ   Y2, Y3, Y2
	VPADDQ   Y2, Y4, Y2
	VMOVDQU  512(CX), Y3
	VMOVDQU  64(DX), Y4
	VPXOR    Y3, Y4, Y4
	VPSHUFD  $0x31, Y4, Y5
	VPMULUDQ Y4, Y5, Y4
	VPSHUFD  $0x4e, Y3, Y3
	VPADDQ   Y1, Y3, Y1
	VPADDQ   Y1, Y4, Y1
	VMOVDQU  544(CX), Y3
	VMOVDQU  96(DX), Y4
	VPXOR    Y3, Y4, Y4
	VPSHUFD  $0x31, Y4, Y5
	VPMULUDQ Y4, Y5, Y4
	VPSHUFD  $0x4e, Y3, Y3
	VPADDQ   Y2, Y3, Y2
	VPADDQ   Y2, Y4, Y2
	VMOVDQU  576(CX), Y3
	VMOVDQU  72(DX), Y4
	VPXOR    Y3, Y4, Y4
	VPSHUFD  $0x31, Y4, Y5
	VPMULUDQ Y4, Y5, Y4
	VPSHUFD  $0x4e, Y3, Y3
	VPADDQ   Y1, Y3, Y1
	VPADDQ   Y1, Y4, Y1
	VMOVDQU  608(CX), Y3
	VMOVDQU  104(DX), Y4
	VPXOR    Y3, Y4, Y4
	VPSHUFD  $0x31, Y4, Y5
	VPMULUDQ Y4, Y5, Y4
	VPSHUFD  $0x4e, Y3, Y3
	VPADDQ   Y2, Y3, Y2
	VPADDQ   Y2, Y4, Y2
	VMOVDQU  640(CX), Y3
	VMOVDQU  80(DX), Y4
	VPXOR    Y3, Y4, Y4
	VPSHUFD  $0x31, Y4, Y5
	VPMULUDQ Y4, Y5, Y4
	VPSHUFD  $0x4e, Y3, Y3
	VPADDQ   Y1, Y3, Y1
	VPADDQ   Y1, Y4, Y1
	VMOVDQU  672(CX), Y3
	VMOVDQU  112(DX), Y4
	VPXOR    Y3, Y4, Y4
	VPSHUFD  $0x31, Y4, Y5
	VPMULUDQ Y4, Y5, Y4
	VPSHUFD  $0x4e, Y3, Y3
	VPADDQ   Y2, Y3, Y2
	VPADDQ   Y2, Y4, Y2
	VMOVDQU  704(CX), Y3
	VMOVDQU  88(DX), Y4
	VPXOR    Y3, Y4, Y4
	VPSHUFD  $0x31, Y4, Y5
	VPMULUDQ Y4, Y5, Y4
	VPSHUFD  $0x4e, Y3, Y3
	VPADDQ   Y1, Y3, Y1
	VPADDQ   Y1, Y4, Y1
	VMOVDQU  736(CX), Y3
	VMOVDQU  120(DX), Y4
	VPXOR    Y3, Y4, Y4
	VPSHUFD  $0x31, Y4, Y5
	VPMULUDQ Y4, Y5, Y4
	VPSHUFD  $0x4e, Y3, Y3
	VPADDQ   Y2, Y3, Y2
	VPADDQ   Y2, Y4, Y2
	VMOVDQU  768(CX), Y3
	VMOVDQU  96(DX), Y4
	VPXOR    Y3, Y4, Y4
	VPSHUFD  $0x31, Y4, Y5
	VPMULUDQ Y4, Y5, Y4
	VPSHUFD  $0x4e, Y3, Y3
	VPADDQ   Y1, Y3, Y1
	VPADDQ   Y1, Y4, Y1
	VMOVDQU  800(CX), Y3
	VMOVDQU  128(DX), Y4
	VPXOR    Y3, Y4, Y4
	VPSHUFD  $0x31, Y4, Y5
	VPMULUDQ Y4, Y5, Y4
	VPSHUFD  $0x4e, Y3, Y3
	VPADDQ   Y2, Y3, Y2
	VPADDQ   Y2, Y4, Y2
	VMOVDQU  832(CX), Y3
	VMOVDQU  104(DX), Y4
	VPXOR    Y3, Y4, Y4
	VPSHUFD  $0x31, Y4, Y5
	VPMULUDQ Y4, Y5, Y4
	VPSHUFD  $0x4e, Y3, Y3
	VPADDQ   Y1, Y3, Y1
	VPADDQ   Y1, Y4, Y1
	VMOVDQU  864(CX), Y3
	VMOVDQU  136(DX), Y4
	VPXOR    Y3, Y4, Y4
	VPSHUFD  $0x31, Y4, Y5
	VPMULUDQ Y4, Y5, Y4
	VPSHUFD  $0x4e, Y3, Y3
	VPADDQ   Y2, Y3, Y2
	VPADDQ   Y2, Y4, Y2
	VMOVDQU  896(CX), Y3
	VMOVDQU  112(DX), Y4
	VPXOR    Y3, Y4, Y4
	VPSHUFD  $0x31, Y4, Y5
	VPMULUDQ Y4, Y5, Y4
	VPSHUFD  $0x4e, Y3, Y3
	VPADDQ   Y1, Y3, Y1
	VPADDQ   Y1, Y4, Y1
	VMOVDQU  928(CX), Y3
	VMOVDQU  144(DX), Y4
	VPXOR    Y3, Y4, Y4
	VPSHUFD  $0x31, Y4, Y5
	VPMULUDQ Y4, Y5, Y4
	VPSHUFD  $0x4e, Y3, Y3
	VPADDQ   Y2, Y3, Y2
	VPADDQ   Y2, Y4, Y2
	VMOVDQU  960(CX), Y3
	VMOVDQU  120(DX), Y4
	VPXOR    Y3, Y4, Y4
	VPSHUFD  $0x31, Y4, Y5
	VPMULUDQ Y4, Y5, Y4
	VPSHUFD  $0x4e, Y3, Y3
	VPADDQ   Y1, Y3, Y1
	VPADDQ   Y1, Y4, Y1
	VMOVDQU  992(CX), Y3
	VMOVDQU  152(DX), Y4
	VPXOR    Y3, Y4, Y4
	VPSHUFD  $0x31, Y4, Y5
	VPMULUDQ Y4, Y5, Y4
	VPSHUFD  $0x4e, Y3, Y3
	VPADDQ   Y2, Y3, Y2
	VPADDQ   Y2, Y4, Y2
	ADDQ     $0x00000400, CX
	SUBQ     $0x00000400, BP
	VPSRLQ   $0x2f, Y1, Y3
	VPXOR    Y1, Y3, Y3
	VPXOR    128(DX), Y3, Y3
	VPMULUDQ Y0, Y3, Y1
	VPSHUFD  $0xf5, Y3, Y3
	VPMULUDQ Y0, Y3, Y3
	VPSLLQ   $0x20, Y3, Y3
	VPADDQ   Y1, Y3, Y1
	VPSRLQ   $0x2f, Y2, Y3
	VPXOR    Y2, Y3, Y3
	VPXOR    160(DX), Y3, Y3
	VPMULUDQ Y0, Y3, Y2
	VPSHUFD  $0xf5, Y3, Y3
	VPMULUDQ Y0, Y3, Y3
	VPSLLQ   $0x20, Y3, Y3
	VPADDQ   Y2, Y3, Y2
	JMP      accum_large

accum:
	CMPQ     BP, $0x40
	JLE      finalize
	VMOVDQU  (CX), Y0
	VMOVDQU  (BX), Y3
	VPXOR    Y0, Y3, Y3
	VPSHUFD  $0x31, Y3, Y4
	VPMULUDQ Y3, Y4, Y3
	VPSHUFD  $0x4e, Y0, Y0
	VPADDQ   Y1, Y0, Y1
	VPADDQ   Y1, Y3, Y1
	VMOVDQU  32(CX), Y0
	VMOVDQU  32(BX), Y3
	VPXOR    Y0, Y3, Y3
	VPSHUFD  $0x31, Y3, Y4
	VPMULUDQ Y3, Y4, Y3
	VPSHUFD  $0x4e, Y0, Y0
	VPADDQ   Y2, Y0, Y2
	VPADDQ   Y2, Y3, Y2
	ADDQ     $0x00000040, CX
	SUBQ     $0x00000040, BP
	ADDQ     $0x00000008, BX
	JMP      accum

finalize:
	CMPQ     BP, $0x00
	JE       return
	SUBQ     $0x40, CX
	ADDQ     BP, CX
	VMOVDQU  (CX), Y0
	VMOVDQU  121(DX), Y3
	VPXOR    Y0, Y3, Y3
	VPSHUFD  $0x31, Y3, Y4
	VPMULUDQ Y3, Y4, Y3
	VPSHUFD  $0x4e, Y0, Y0
	VPADDQ   Y1, Y0, Y1
	VPADDQ   Y1, Y3, Y1
	VMOVDQU  32(CX), Y0
	VMOVDQU  153(DX), Y3
	VPXOR    Y0, Y3, Y3
	VPSHUFD  $0x31, Y3, Y4
	VPMULUDQ Y3, Y4, Y3
	VPSHUFD  $0x4e, Y0, Y0
	VPADDQ   Y2, Y0, Y2
	VPADDQ   Y2, Y3, Y2

return:
	VMOVDQU Y1, (AX)
	VMOVDQU Y2, 32(AX)
	RET
